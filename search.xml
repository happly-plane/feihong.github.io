<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>LLM微调实践</title>
    <url>/2024/08/28/LLM%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[<h2 id="安装相关库"><a href="#安装相关库" class="headerlink" title="安装相关库"></a>安装相关库</h2><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">pip install transformers</span><br><span class="line">pip install accelerate</span><br><span class="line">pip install huggingface_hub</span><br><span class="line">pip install ipywidgets</span><br><span class="line">pip install datasets</span><br><span class="line">pip install trl</span><br><span class="line">pip install peft</span><br><span class="line">pip install swanlab</span><br></pre></td></tr></table></figure></div>
<h2 id="下载模型与数据集"><a href="#下载模型与数据集" class="headerlink" title="下载模型与数据集"></a>下载模型与数据集</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;HF_ENDPOINT&#x27;</span>] = <span class="string">&#x27;https://hf-mirror.com&#x27;</span></span><br><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> snapshot_download</span><br></pre></td></tr></table></figure></div>
<ul>
<li>注意HF_ENDPOINT环境变量需在huggingface相关库之前，否则不生效</li>
</ul>
<h2 id="然后下载模型与数据集"><a href="#然后下载模型与数据集" class="headerlink" title="然后下载模型与数据集"></a>然后下载模型与数据集</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">snapshot_download(repo_id=<span class="string">&quot;b-mc2/sql-create-context&quot;</span>,</span><br><span class="line">                  repo_type=<span class="string">&quot;dataset&quot;</span>,</span><br><span class="line">                  local_dir=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">                  max_workers=<span class="number">8</span>,</span><br><span class="line">                  resume_download=<span class="literal">True</span></span><br><span class="line">                  )</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">snapshot_download(</span><br><span class="line">    repo_id=<span class="string">&quot;codellama/CodeLlama-7b-hf&quot;</span>,</span><br><span class="line">    local_dir=<span class="string">&quot;CodeLlama-7b-hf&quot;</span>,</span><br><span class="line">    resume_download=<span class="literal">True</span>,</span><br><span class="line">    max_workers=<span class="number">8</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<p>以下是训练代码</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">import</span> swanlab</span><br><span class="line"><span class="keyword">from</span> swanlab.integration.huggingface <span class="keyword">import</span> SwanLabCallback</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> SFTTrainer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> setup_chat_format</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> swanlab</span><br><span class="line"></span><br><span class="line">swanlab.login(api_key=<span class="string">&#x27;2mVyoFagmXbwiICMAWx6O&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model_id = <span class="string">&quot;internlm2_5-1_8b-chat&quot;</span> <span class="comment"># or `mistralai/Mistral-7B-v0.1`</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;json&quot;</span>,data_files=<span class="string">&quot;train_dataset.json&quot;</span>,split=<span class="string">&quot;train&quot;</span>,)</span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_id,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line"><span class="comment">#     attn_implementation=&quot;flash_attention_2&quot;,</span></span><br><span class="line">    torch_dtype=torch.bfloat16,</span><br><span class="line">    trust_remote_code=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">tokenizer.padding_side = <span class="string">&#x27;right&#x27;</span> <span class="comment"># 以防止警告</span></span><br><span class="line"></span><br><span class="line">model, tokenizer = setup_chat_format(model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于QLoRA论文和Sebastian Raschka实验的LoRA配置</span></span><br><span class="line">peft_config = LoraConfig(</span><br><span class="line">        lora_alpha=<span class="number">128</span>,</span><br><span class="line">        lora_dropout=<span class="number">0.05</span>,</span><br><span class="line">        r=<span class="number">256</span>,</span><br><span class="line">        bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">        target_modules=<span class="string">&quot;all-linear&quot;</span>,</span><br><span class="line">        task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;code-llama-7b-text-to-sql&quot;</span>, <span class="comment"># 要保存的目录和存储库ID</span></span><br><span class="line">    num_train_epochs=<span class="number">3</span>,                     <span class="comment"># 训练周期数</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">3</span>,          <span class="comment"># 训练期间每个设备的批量大小</span></span><br><span class="line">    gradient_accumulation_steps=<span class="number">2</span>,          <span class="comment"># 反向/更新前的步骤数</span></span><br><span class="line">    gradient_checkpointing=<span class="literal">True</span>,            <span class="comment"># 使用渐变检查点来节省内存</span></span><br><span class="line">    optim=<span class="string">&quot;adamw_torch&quot;</span>,              <span class="comment"># 使用融合的adamw优化器</span></span><br><span class="line">    logging_steps=<span class="number">10</span>,                       <span class="comment"># 每10步记录一次</span></span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,                  <span class="comment"># 每个epoch保存检查点</span></span><br><span class="line">    learning_rate=<span class="number">2e-4</span>,                     <span class="comment"># 学习率，基于QLoRA论文</span></span><br><span class="line"><span class="comment">#     bf16=True,                              # 使用bfloat16精度</span></span><br><span class="line">    max_grad_norm=<span class="number">0.3</span>,                      <span class="comment"># 基于QLoRA论文的最大梯度范数</span></span><br><span class="line">    warmup_ratio=<span class="number">0.03</span>,                      <span class="comment"># 根据QLoRA论文的预热比例</span></span><br><span class="line">    lr_scheduler_type=<span class="string">&quot;constant&quot;</span>,           <span class="comment"># 使用恒定学习率调度器</span></span><br><span class="line">    report_to=<span class="string">&quot;tensorboard&quot;</span>,                <span class="comment"># 将指标报告到Tensorboard</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">swanlab_callback = SwanLabCallback(experiment_name=<span class="string">&quot;TransformersTest&quot;</span>, cloud=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">max_seq_length = <span class="number">3072</span> <span class="comment"># 数据集模型和打包的最大序列长度</span></span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=args,</span><br><span class="line">    train_dataset=dataset,</span><br><span class="line">    peft_config=peft_config,</span><br><span class="line">    max_seq_length=max_seq_length,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    callbacks=[swanlab_callback],</span><br><span class="line">    packing=<span class="literal">True</span>,</span><br><span class="line">    dataset_kwargs=&#123;</span><br><span class="line">        <span class="string">&quot;add_special_tokens&quot;</span>: <span class="literal">False</span>,  <span class="comment"># 我们使用特殊 tokens</span></span><br><span class="line">        <span class="string">&quot;append_concat_token&quot;</span>: <span class="literal">False</span>, <span class="comment"># 不需要添加额外的分隔符 token</span></span><br><span class="line">    &#125;)</span><br><span class="line">trainer.train()</span><br><span class="line">trainer.save_model()</span><br></pre></td></tr></table></figure></div>
]]></content>
      <categories>
        <category>LLM</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/08/28/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a class="link"   href="https://hexo.io/" >Hexo <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>! This is your very first post. Check <a class="link"   href="https://hexo.io/docs/" >documentation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class="link"   href="https://hexo.io/docs/troubleshooting.html" >troubleshooting <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> or you can ask me on <a class="link"   href="https://github.com/hexojs/hexo/issues" >GitHub <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/writing.html" >Writing <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/server.html" >Server <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/generating.html" >Generating <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/one-command-deployment.html" >Deployment <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
]]></content>
  </entry>
</search>
